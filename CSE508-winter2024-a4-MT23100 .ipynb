{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8201032,"sourceType":"datasetVersion","datasetId":4858341}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**\nInstalling the Transformers library via pip allows you to access powerful tools and models for natural language processing tasks, including GPT-2. It's a standard step in setting up projects involving text data analysis and processing.**","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:14.290090Z","iopub.execute_input":"2024-04-22T11:10:14.290930Z","iopub.status.idle":"2024-04-22T11:10:26.654931Z","shell.execute_reply.started":"2024-04-22T11:10:14.290899Z","shell.execute_reply":"2024-04-22T11:10:26.653694Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelWithLMHead\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:26.657016Z","iopub.execute_input":"2024-04-22T11:10:26.657327Z","iopub.status.idle":"2024-04-22T11:10:26.663683Z","shell.execute_reply.started":"2024-04-22T11:10:26.657297Z","shell.execute_reply":"2024-04-22T11:10:26.662815Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:26.664680Z","iopub.execute_input":"2024-04-22T11:10:26.664946Z","iopub.status.idle":"2024-04-22T11:10:26.684719Z","shell.execute_reply.started":"2024-04-22T11:10:26.664924Z","shell.execute_reply":"2024-04-22T11:10:26.683685Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}]},{"cell_type":"code","source":"import csv\nreviews_path = \"/kaggle/input/final-dataset-12334/Reviews.csv\"\n\nreviews = []\n\n# Open the CSV file in read mode with UTF-8 encoding\nwith open(reviews_path, \"r\", encoding=\"utf-8\") as reviews_raw:\n    # Create a CSV reader object\n    csv_reader = csv.DictReader(reviews_raw)\n    \n    # Iterate over each row in the CSV file\n    for row in csv_reader:\n        # Extract the summary and text from the current row\n        summary = row[\"Summary\"]\n        text = row[\"Text\"]\n        \n        # Replace \" = \" with \" TL;DR \" in summary and text\n        summary = summary.replace(\" = \", \" TL;DR \")\n        text = text.replace(\" = \", \" TL;DR \")\n        \n        # Combine summary and text into one string\n        review = f\"{text.strip()} = {summary.strip()}\\n\"\n        \n        # Append the combined review to the list\n        reviews.append(review)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:26.686567Z","iopub.execute_input":"2024-04-22T11:10:26.686867Z","iopub.status.idle":"2024-04-22T11:10:32.608248Z","shell.execute_reply.started":"2024-04-22T11:10:26.686840Z","shell.execute_reply":"2024-04-22T11:10:32.607443Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"reviews[:3]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:32.609354Z","iopub.execute_input":"2024-04-22T11:10:32.609639Z","iopub.status.idle":"2024-04-22T11:10:32.615889Z","shell.execute_reply.started":"2024-04-22T11:10:32.609614Z","shell.execute_reply":"2024-04-22T11:10:32.614977Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"['I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most. = Good Quality Dog Food\\n',\n 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\". = Not as Advertised\\n',\n 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch. = \"Delight\" says it all\\n',\n 'If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal. = Cough Medicine\\n',\n 'Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal. = Great taffy\\n']"},"metadata":{}}]},{"cell_type":"code","source":"len(reviews)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:32.617044Z","iopub.execute_input":"2024-04-22T11:10:32.617330Z","iopub.status.idle":"2024-04-22T11:10:32.633746Z","shell.execute_reply.started":"2024-04-22T11:10:32.617306Z","shell.execute_reply":"2024-04-22T11:10:32.632949Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"568454"},"metadata":{}}]},{"cell_type":"code","source":"reviews[5]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:32.636045Z","iopub.execute_input":"2024-04-22T11:10:32.636337Z","iopub.status.idle":"2024-04-22T11:10:32.646142Z","shell.execute_reply.started":"2024-04-22T11:10:32.636313Z","shell.execute_reply":"2024-04-22T11:10:32.645403Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"\"I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service! = The Best Hot Sauce in the World\\n\""},"metadata":{}}]},{"cell_type":"code","source":"# Calculate the average length of reviews in terms of the number of words\n\n# Calculate the total length of all reviews by summing the number of words in each review\ntotal_length = sum([len(review.split()) for review in reviews])\n\n# Calculate the average length by dividing the total length by the number of reviews\n# len(reviews) returns the number of reviews\navg_length = total_length / len(reviews)\n\n# Display \navg_length\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:32.647080Z","iopub.execute_input":"2024-04-22T11:10:32.647312Z","iopub.status.idle":"2024-04-22T11:10:35.709695Z","shell.execute_reply.started":"2024-04-22T11:10:32.647292Z","shell.execute_reply":"2024-04-22T11:10:35.708806Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"85.37717211946789"},"metadata":{}}]},{"cell_type":"code","source":"max_length = 100","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:35.710747Z","iopub.execute_input":"2024-04-22T11:10:35.711021Z","iopub.status.idle":"2024-04-22T11:10:35.715213Z","shell.execute_reply.started":"2024-04-22T11:10:35.710998Z","shell.execute_reply":"2024-04-22T11:10:35.714115Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelWithLMHead\n\n# Import necessary classes from the Transformers library\n\n# Initialize tokenizer with pre-trained weights from GPT-2 model\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n# Initialize GPT-2 model with pre-trained weights\nmodel = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n\n# Move the model to the specified device (e.g., GPU)\nmodel = model.to(device)\n\n# Initialize AdamW optimizer for updating model parameters during training\noptimizer = optim.AdamW(model.parameters(), lr=3e-4)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:35.716304Z","iopub.execute_input":"2024-04-22T11:10:35.716572Z","iopub.status.idle":"2024-04-22T11:10:39.772269Z","shell.execute_reply.started":"2024-04-22T11:10:35.716549Z","shell.execute_reply":"2024-04-22T11:10:39.771289Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"tokenizer.encode(\" TL;DR \")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:39.923286Z","iopub.execute_input":"2024-04-22T11:10:39.923586Z","iopub.status.idle":"2024-04-22T11:10:39.934572Z","shell.execute_reply.started":"2024-04-22T11:10:39.923563Z","shell.execute_reply":"2024-04-22T11:10:39.933734Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"[24811, 26, 7707, 220]"},"metadata":{}}]},{"cell_type":"code","source":"# Calculate the length of the tokenized representation of the string \" TL;DR \"\nextra_length = len(tokenizer.encode(\" TL;DR \")) \n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:39.935680Z","iopub.execute_input":"2024-04-22T11:10:39.935972Z","iopub.status.idle":"2024-04-22T11:10:39.944048Z","shell.execute_reply.started":"2024-04-22T11:10:39.935949Z","shell.execute_reply":"2024-04-22T11:10:39.943291Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"class ReviewDataset(Dataset):\n    def __init__(self, tokenizer, reviews, max_len):\n        # Initialize dataset with tokenizer, reviews, and maximum sequence length\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.eos = self.tokenizer.eos_token  # End of sequence token\n        self.eos_id = self.tokenizer.eos_token_id  # End of sequence token ID\n        self.reviews = reviews\n        self.result = []\n\n        # Process each review in the dataset\n        for review in self.reviews:\n            # Encode the review text using tokenizer.encode() and add end of sequence token\n            tokenized = self.tokenizer.encode(review + self.eos)\n            \n            # Pad/truncate the encoded sequence to max_len\n            padded = self.pad_truncate(tokenized)            \n\n            # Convert the padded sequence to a PyTorch tensor and add to the result list\n            self.result.append(torch.tensor(padded))\n\n    def __len__(self):\n        # Return the total number of samples in the dataset\n        return len(self.result)\n\n    def __getitem__(self, item):\n        # Retrieve a sample from the dataset by index\n        return self.result[item]\n\n    def pad_truncate(self, name):\n        # Pad or truncate the input sequence to match the specified max_len\n        \n        # Calculate the length of the tokenized review, excluding the length of \" TL;DR \"\n        name_length = len(name) - extra_length\n        \n        if name_length < self.max_len:\n            # If the length is less than max_len, pad the sequence with EOS tokens\n            difference = self.max_len - name_length\n            result = name + [self.eos_id] * difference\n        elif name_length > self.max_len:\n            # If the length exceeds max_len, truncate the sequence and add EOS token\n            result = name[:self.max_len + 3] + [self.eos_id]  # Adding 3 to account for extra tokens added during padding\n        else:\n            # If the length matches max_len, return the original sequence\n            result = name\n        \n        return result\n\n# Create an instance of the ReviewDataset class\ndataset = ReviewDataset(tokenizer, reviews, max_length)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:10:43.021611Z","iopub.execute_input":"2024-04-22T11:10:43.022279Z","iopub.status.idle":"2024-04-22T11:10:43.031594Z","shell.execute_reply.started":"2024-04-22T11:10:43.022244Z","shell.execute_reply":"2024-04-22T11:10:43.030688Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"# DATALOADER","metadata":{}},{"cell_type":"code","source":"# Create a DataLoader to batch and shuffle the dataset for training\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n\n# Define a function for training the model\ndef train(model, optimizer, dl, epochs):\n    # Iterate over the specified number of epochs\n    for epoch in range(epochs):\n        # Iterate over each batch in the DataLoader\n        for idx, batch in enumerate(dl):\n            # Enable gradient computation for the current batch\n            with torch.set_grad_enabled(True):\n                # Reset gradients to zero\n                optimizer.zero_grad()\n                \n                # Move the batch to the specified device (e.g., GPU)\n                batch = batch.to(device)\n                \n                # Forward pass: compute model predictions\n                output = model(batch, labels=batch)\n                \n                # Compute the loss based on model predictions\n                loss = output[0]\n                \n                # Backward pass: compute gradients and update model parameters\n                loss.backward()\n                optimizer.step()\n                \n                # Print loss every 50 batches\n                if idx % 50 == 0:\n                    print(\"loss: %f, %d\"%(loss, idx))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:15:35.372059Z","iopub.execute_input":"2024-04-22T11:15:35.372399Z","iopub.status.idle":"2024-04-22T11:15:35.377393Z","shell.execute_reply.started":"2024-04-22T11:15:35.372369Z","shell.execute_reply":"2024-04-22T11:15:35.376511Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# Initiate the training process for the specified model, optimizer, dataloader, and number of epochs\ntrain(model=model, optimizer=optimizer, dl=dataloader, epochs=1)\n\n# The train function iterates over each epoch and within each epoch, it processes batches of data.\n# For each batch, it computes the model output, calculates the loss, and updates the model parameters based on the optimizer.\n# The loss value is printed every 50 batches to monitor the training progress.\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T11:15:35.395036Z","iopub.execute_input":"2024-04-22T11:15:35.395333Z","iopub.status.idle":"2024-04-22T13:32:17.596943Z","shell.execute_reply.started":"2024-04-22T11:15:35.395308Z","shell.execute_reply":"2024-04-22T13:32:17.596086Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"loss: 7.179998, 0\nloss: 2.546208, 50\nloss: 2.173442, 100\nloss: 2.687057, 150\nloss: 2.299516, 200\nloss: 2.323481, 250\nloss: 2.308963, 300\nloss: 2.470415, 350\nloss: 2.384151, 400\nloss: 2.411604, 450\nloss: 2.191579, 500\nloss: 2.710379, 550\nloss: 2.356698, 600\nloss: 2.104545, 650\nloss: 2.092360, 700\nloss: 2.215420, 750\nloss: 2.406816, 800\nloss: 2.368682, 850\nloss: 2.637617, 900\nloss: 2.189571, 950\nloss: 2.308575, 1000\nloss: 2.238402, 1050\nloss: 2.063868, 1100\nloss: 2.344211, 1150\nloss: 2.464975, 1200\nloss: 2.153612, 1250\nloss: 2.138024, 1300\nloss: 2.524029, 1350\nloss: 2.249254, 1400\nloss: 2.285464, 1450\nloss: 2.017530, 1500\nloss: 1.820162, 1550\nloss: 2.174565, 1600\nloss: 2.552882, 1650\nloss: 2.148380, 1700\nloss: 2.165045, 1750\nloss: 2.298380, 1800\nloss: 2.125666, 1850\nloss: 2.262254, 1900\nloss: 2.293321, 1950\nloss: 2.087833, 2000\nloss: 2.001413, 2050\nloss: 2.193791, 2100\nloss: 2.232748, 2150\nloss: 2.245459, 2200\nloss: 2.175688, 2250\nloss: 2.203779, 2300\nloss: 2.333107, 2350\nloss: 2.174418, 2400\nloss: 2.005616, 2450\nloss: 2.140307, 2500\nloss: 2.320475, 2550\nloss: 2.034000, 2600\nloss: 1.952949, 2650\nloss: 2.201778, 2700\nloss: 1.886993, 2750\nloss: 2.196239, 2800\nloss: 2.075902, 2850\nloss: 2.085238, 2900\nloss: 2.418180, 2950\nloss: 2.034922, 3000\nloss: 1.908317, 3050\nloss: 2.503305, 3100\nloss: 2.476161, 3150\nloss: 2.130602, 3200\nloss: 2.035571, 3250\nloss: 2.077102, 3300\nloss: 2.369753, 3350\nloss: 2.246732, 3400\nloss: 2.122199, 3450\nloss: 2.375435, 3500\nloss: 2.134913, 3550\nloss: 2.200869, 3600\nloss: 2.202316, 3650\nloss: 2.124436, 3700\nloss: 2.127747, 3750\nloss: 2.236326, 3800\nloss: 2.151803, 3850\nloss: 1.962897, 3900\nloss: 2.054335, 3950\nloss: 1.915839, 4000\nloss: 2.030221, 4050\nloss: 2.170536, 4100\nloss: 1.951652, 4150\nloss: 2.320496, 4200\nloss: 2.102848, 4250\nloss: 2.255838, 4300\nloss: 2.369204, 4350\nloss: 2.084755, 4400\nloss: 2.397063, 4450\nloss: 1.934528, 4500\nloss: 2.214739, 4550\nloss: 2.275740, 4600\nloss: 1.953343, 4650\nloss: 1.905339, 4700\nloss: 2.274620, 4750\nloss: 2.203720, 4800\nloss: 2.447945, 4850\nloss: 2.073444, 4900\nloss: 2.244152, 4950\nloss: 2.092780, 5000\nloss: 2.122076, 5050\nloss: 2.215444, 5100\nloss: 2.002038, 5150\nloss: 2.227186, 5200\nloss: 2.073352, 5250\nloss: 1.994648, 5300\nloss: 2.147708, 5350\nloss: 2.171522, 5400\nloss: 2.021449, 5450\nloss: 2.277795, 5500\nloss: 2.065826, 5550\nloss: 2.006109, 5600\nloss: 2.222303, 5650\nloss: 2.145988, 5700\nloss: 1.988699, 5750\nloss: 2.416167, 5800\nloss: 2.175748, 5850\nloss: 2.003775, 5900\nloss: 2.046824, 5950\nloss: 2.189290, 6000\nloss: 2.265274, 6050\nloss: 1.808317, 6100\nloss: 1.829223, 6150\nloss: 1.955007, 6200\nloss: 2.240872, 6250\nloss: 2.235186, 6300\nloss: 2.041268, 6350\nloss: 2.035798, 6400\nloss: 2.236791, 6450\nloss: 2.020753, 6500\nloss: 2.066976, 6550\nloss: 1.782139, 6600\nloss: 2.219458, 6650\nloss: 2.132128, 6700\nloss: 2.053517, 6750\nloss: 2.033494, 6800\nloss: 2.018213, 6850\nloss: 2.059720, 6900\nloss: 2.386530, 6950\nloss: 2.163881, 7000\nloss: 1.997306, 7050\nloss: 2.115539, 7100\nloss: 2.318241, 7150\nloss: 2.365331, 7200\nloss: 2.207479, 7250\nloss: 2.244404, 7300\nloss: 2.063596, 7350\nloss: 2.148598, 7400\nloss: 2.168213, 7450\nloss: 2.177212, 7500\nloss: 1.904301, 7550\nloss: 2.093769, 7600\nloss: 1.978347, 7650\nloss: 2.098054, 7700\nloss: 2.169493, 7750\nloss: 2.085661, 7800\nloss: 1.886774, 7850\nloss: 2.510666, 7900\nloss: 1.984088, 7950\nloss: 1.963435, 8000\nloss: 2.218378, 8050\nloss: 2.078304, 8100\nloss: 2.104980, 8150\nloss: 2.195798, 8200\nloss: 1.712596, 8250\nloss: 1.887635, 8300\nloss: 2.009505, 8350\nloss: 2.199556, 8400\nloss: 2.034193, 8450\nloss: 2.181220, 8500\nloss: 1.786736, 8550\nloss: 1.870662, 8600\nloss: 1.907265, 8650\nloss: 1.980892, 8700\nloss: 1.855423, 8750\nloss: 2.065232, 8800\nloss: 2.271160, 8850\nloss: 2.212322, 8900\nloss: 2.073380, 8950\nloss: 2.051777, 9000\nloss: 2.061492, 9050\nloss: 2.065233, 9100\nloss: 2.000200, 9150\nloss: 2.214236, 9200\nloss: 1.750447, 9250\nloss: 2.013983, 9300\nloss: 1.929532, 9350\nloss: 1.751237, 9400\nloss: 2.284103, 9450\nloss: 2.029752, 9500\nloss: 2.249203, 9550\nloss: 2.154821, 9600\nloss: 1.940630, 9650\nloss: 1.949405, 9700\nloss: 2.269420, 9750\nloss: 2.016193, 9800\nloss: 1.720772, 9850\nloss: 2.148767, 9900\nloss: 1.938862, 9950\nloss: 1.986837, 10000\nloss: 1.899756, 10050\nloss: 2.005066, 10100\nloss: 2.186970, 10150\nloss: 1.826530, 10200\nloss: 1.976253, 10250\nloss: 1.915622, 10300\nloss: 2.445463, 10350\nloss: 1.949667, 10400\nloss: 2.033354, 10450\nloss: 2.000247, 10500\nloss: 2.143237, 10550\nloss: 1.976022, 10600\nloss: 2.054891, 10650\nloss: 1.927113, 10700\nloss: 1.998320, 10750\nloss: 2.214032, 10800\nloss: 1.865538, 10850\nloss: 2.020538, 10900\nloss: 1.775335, 10950\nloss: 2.042610, 11000\nloss: 2.026172, 11050\nloss: 1.941727, 11100\nloss: 1.800531, 11150\nloss: 2.087330, 11200\nloss: 1.973143, 11250\nloss: 1.870570, 11300\nloss: 2.059785, 11350\nloss: 2.085273, 11400\nloss: 2.109655, 11450\nloss: 1.994739, 11500\nloss: 1.864831, 11550\nloss: 1.718140, 11600\nloss: 1.817069, 11650\nloss: 2.133237, 11700\nloss: 1.800212, 11750\nloss: 2.162971, 11800\nloss: 2.038387, 11850\nloss: 1.986510, 11900\nloss: 2.081030, 11950\nloss: 1.835257, 12000\nloss: 2.328672, 12050\nloss: 1.724025, 12100\nloss: 1.751005, 12150\nloss: 1.877854, 12200\nloss: 1.936718, 12250\nloss: 1.924222, 12300\nloss: 1.992618, 12350\nloss: 1.995885, 12400\nloss: 2.182284, 12450\nloss: 2.126747, 12500\nloss: 1.975394, 12550\nloss: 1.836168, 12600\nloss: 2.267316, 12650\nloss: 1.854945, 12700\nloss: 2.052965, 12750\nloss: 1.885056, 12800\nloss: 2.154880, 12850\nloss: 1.799294, 12900\nloss: 1.945950, 12950\nloss: 2.175445, 13000\nloss: 1.725501, 13050\nloss: 1.912111, 13100\nloss: 1.566902, 13150\nloss: 2.184010, 13200\nloss: 1.778355, 13250\nloss: 1.827977, 13300\nloss: 1.982434, 13350\nloss: 1.919181, 13400\nloss: 1.851663, 13450\nloss: 1.915917, 13500\nloss: 2.049237, 13550\nloss: 1.614667, 13600\nloss: 1.951115, 13650\nloss: 1.971087, 13700\nloss: 2.019321, 13750\nloss: 1.785248, 13800\nloss: 1.938199, 13850\nloss: 1.888795, 13900\nloss: 1.992351, 13950\nloss: 2.044557, 14000\nloss: 2.053672, 14050\nloss: 2.098352, 14100\nloss: 2.236063, 14150\nloss: 1.881279, 14200\nloss: 2.199512, 14250\nloss: 1.765493, 14300\nloss: 2.105541, 14350\nloss: 2.120003, 14400\nloss: 2.153830, 14450\nloss: 1.797022, 14500\nloss: 1.759722, 14550\nloss: 1.919263, 14600\nloss: 1.950946, 14650\nloss: 1.915567, 14700\nloss: 1.879333, 14750\nloss: 1.876890, 14800\nloss: 2.255475, 14850\nloss: 1.770617, 14900\nloss: 1.494443, 14950\nloss: 1.816499, 15000\nloss: 1.873335, 15050\nloss: 2.096269, 15100\nloss: 1.895371, 15150\nloss: 2.235538, 15200\nloss: 1.967900, 15250\nloss: 2.165826, 15300\nloss: 1.865141, 15350\nloss: 1.840579, 15400\nloss: 1.992269, 15450\nloss: 2.109605, 15500\nloss: 1.873654, 15550\nloss: 1.867358, 15600\nloss: 1.997511, 15650\nloss: 1.770107, 15700\nloss: 1.756370, 15750\nloss: 2.224028, 15800\nloss: 1.503791, 15850\nloss: 2.109290, 15900\nloss: 1.984638, 15950\nloss: 1.936308, 16000\nloss: 1.872335, 16050\nloss: 1.899984, 16100\nloss: 1.735803, 16150\nloss: 1.835989, 16200\nloss: 2.043731, 16250\nloss: 1.798785, 16300\nloss: 1.807341, 16350\nloss: 1.965681, 16400\nloss: 1.795312, 16450\nloss: 1.939250, 16500\nloss: 1.690184, 16550\nloss: 1.644580, 16600\nloss: 2.069916, 16650\nloss: 2.255896, 16700\nloss: 1.822145, 16750\nloss: 2.038384, 16800\nloss: 2.153052, 16850\nloss: 2.011309, 16900\nloss: 1.799299, 16950\nloss: 2.077619, 17000\nloss: 2.137277, 17050\nloss: 2.003010, 17100\nloss: 1.854963, 17150\nloss: 1.886199, 17200\nloss: 1.904608, 17250\nloss: 1.798188, 17300\nloss: 1.999772, 17350\nloss: 1.607766, 17400\nloss: 2.025405, 17450\nloss: 2.107935, 17500\nloss: 2.026344, 17550\nloss: 1.773535, 17600\nloss: 2.227053, 17650\nloss: 2.090322, 17700\nloss: 1.871062, 17750\n","output_type":"stream"}]},{"cell_type":"code","source":"def topk(probs, n=9):\n    # Softmax the scores to convert them into probabilities\n    probs = torch.softmax(probs, dim=-1)\n    \n    # Use PyTorch's topk method to get the top k probabilities and their corresponding indices\n    tokensProb, topIx = torch.topk(probs, k=n)\n    \n    # Normalize the new selection pool (9 choices)\n    tokensProb = tokensProb / torch.sum(tokensProb)\n\n    # Send to CPU for numpy handling\n    tokensProb = tokensProb.cpu().detach().numpy()\n\n    # Make a random choice from the pool based on the new probability distribution\n    choice = np.random.choice(n, 1, p=tokensProb)\n    tokenId = topIx[choice][0]\n\n    return int(tokenId)\n\ndef model_infer(model, tokenizer, review, max_length=15):\n    # Preprocess the initial token (task designator)\n    review_encoded = tokenizer.encode(review)\n    result = review_encoded\n    initial_input = torch.tensor(review_encoded).unsqueeze(0).to(device)\n\n    with torch.set_grad_enabled(False):\n        # Feed the initial token to the model\n        output = model(initial_input)\n\n        # Flatten the logits at the final time step\n        logits = output.logits[0, -1]\n\n        # Make a top-k choice and append to the result\n        result.append(topk(logits))\n\n        # For max_length times:\n        for _ in range(max_length):\n            # Feed the current sequence to the model and make a choice\n            input = torch.tensor(result).unsqueeze(0).to(device)\n            output = model(input)\n            logits = output.logits[0, -1]\n            res_id = topk(logits)\n\n            # If the chosen token is EOS, return the result\n            if res_id == tokenizer.eos_token_id:\n                return tokenizer.decode(result)\n            else:  # Append to the sequence\n                result.append(res_id)\n    # If no EOS is generated, return after reaching the max_len\n    return tokenizer.decode(result)\n\n# The topk function selects the top k tokens based on their probabilities and returns one of them randomly.\n# The model_infer function generates a sequence of tokens using the given model and tokenizer, starting with the provided review.\n# It iteratively predicts the next token based on the previous sequence and stops when the EOS token is encountered or after reaching the max_length.\n# This function returns the generated sequence as a decoded string.\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:44:51.556996Z","iopub.execute_input":"2024-04-22T15:44:51.557870Z","iopub.status.idle":"2024-04-22T15:44:51.564220Z","shell.execute_reply.started":"2024-04-22T15:44:51.557835Z","shell.execute_reply":"2024-04-22T15:44:51.563315Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# Select a random sample of 5 reviews from the list of reviews and split each review at \" TL;DR \" to extract the original review text\nsample_reviews = [review.split(\" TL;DR \")[0] for review in random.sample(reviews, 5)]\n\n# Display the sampled review texts\nsample_reviews\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:46:00.150629Z","iopub.execute_input":"2024-04-22T15:46:00.151460Z","iopub.status.idle":"2024-04-22T15:46:00.158070Z","shell.execute_reply.started":"2024-04-22T15:46:00.151426Z","shell.execute_reply":"2024-04-22T15:46:00.157142Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"[\"I love Coconut and the flavor is really good, but it was much sweeter than expected. I like to have a healthy mid morning snack at work and this is just a little too much to serve that purpose. I have enjoyed having one before a workout to get some extra energy but I won't be buying these on a regular basis. = YUMMY, but too sweet for an Every Day snack\\n\",\n 'There\\'s no getting around the fact that this is canned meat and gravy, and so take that into account.  It\\'s going to have a bit of canned flavor, smell and texture -- not like Grandma\\'s homemade pot roast.<br /><br />But considering that, this is a convenient and pretty well done product for a canned food.  There is a good balance of roast beef chunks to gravy.  I would say you definitely get two servings per can and, depending on use, three or even four servings.  (I can\\'t remember what the can claims as number of servings but I think it was two per can.)  A hungry person or big eater might want to consume the entire can.<br /><br />This isn\\'t a product where you\\'d buy it just for the roast beef, draining off the liquid; this is intended to include the gravy as part of the overall dish.  That makes it convenient for things like topping a baked potato, mashed potatoes, grits, toast, rice, pasta, or whatever you like.  But it\\'s specific in that sense.  If you need canned beef for other purposes, you will want to buy another different product.<br /><br />The roast beef is tender and tastes good.  I haven\\'t encountered any fat or grisly bits.  The beef chunks tend to be around 1/2\" to 1\", roughly, and most of them are pretty consistent in size.  The gravy, while slightly \"canned\" for my taste, is still okay.  The product is not overly salted, which is a big plus.<br /><br />Besides the obvious uses, you could use this as simply beef gravy if you wanted, by breaking up the chunks into smaller bits.  The chunks flake into bits easily.  That might be good if you are feeding to young children and don\\'t want to worry about them gacking on a big piece of meat.  But it also means that you could use this as a casserole sauce or binder for mixing up rice dishes etc. = Definitely Canned Food, But Not Bad Considering\\n',\n 'Thank you, THANK YOU, THANK YOU!  Obsolutely THE BEST!!! LOVED IT.  BOUGHT more because now I stopped drinking all the other coffee but this beautiful Mahmet Efendi Turkish Coffee!!!!!! = Obsolutely the BEST TURKISH COFFEE!!!!\\n',\n 'Simply delectable assortment. The only negative I can think of is the lack of an assortment guide. But, I guess that adds to the surprise. The package is beautifully and lovingly packaged, and my chocolates arrived without melting! Great service! The chocolates themselves melt in your mouth and their coffee-hazlenut pralines are to die for! The hazlenut is just that-pure, unadulterated nuts-that said, some people might expect ferrero rocher like taste-its very different, and for some people, even distasteful. But I love it! There are around three layers in the box, and each layer has around seven pieces. Pretty good value actually, and makes an excellent gift! = Sinfully Satisfying\\n',\n 'Do not buy the sugar free versions! They impart a strong chemical after taste. Kids hated it and never finished the snowballs. = Awefull !!!\\n']"},"metadata":{}}]},{"cell_type":"code","source":"# Iterate over each review in the sample_reviews list\nfor review in sample_reviews:\n    # Initialize a set to store unique summaries generated for each review\n    summaries = set()\n    \n    # Print the original review text\n    print(review)\n    \n    # Generate summaries until there are at least 3 unique summaries\n    while len(summaries) < 3:\n        # Generate a summary for the current review using the model_infer function\n        # Append \" TL;DR \" to the review to indicate the start of summary generation\n        summary = model_infer(model, tokenizer, review + \" TL;DR \").split(\" TL;DR \")[1].strip()\n        \n        # Add the generated summary to the set of summaries if it's not already present\n        if summary not in summaries:\n            summaries.add(summary)\n    \n    # Print the generated summaries for the current review\n    print(\"Summaries: \" + str(summaries) + \"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:47:23.911020Z","iopub.execute_input":"2024-04-22T15:47:23.911796Z","iopub.status.idle":"2024-04-22T15:47:23.970332Z","shell.execute_reply.started":"2024-04-22T15:47:23.911754Z","shell.execute_reply":"2024-04-22T15:47:23.969171Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"I love Coconut and the flavor is really good, but it was much sweeter than expected. I like to have a healthy mid morning snack at work and this is just a little too much to serve that purpose. I have enjoyed having one before a workout to get some extra energy but I won't be buying these on a regular basis. = YUMMY, but too sweet for an Every Day snack\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[91], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(review)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(summaries) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m----> 5\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreview\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m TL;DR \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m TL;DR \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m summary \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m summaries:\n\u001b[1;32m      7\u001b[0m         summaries\u001b[38;5;241m.\u001b[39madd(summary)\n","Cell \u001b[0;32mIn[88], line 9\u001b[0m, in \u001b[0;36mmodel_infer\u001b[0;34m(model, tokenizer, review, max_length)\u001b[0m\n\u001b[1;32m      5\u001b[0m initial_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(review_encoded)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Feed the init token to the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Flatten the logits at the final time step\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"],"ename":"TypeError","evalue":"'NoneType' object is not callable","output_type":"error"}]},{"cell_type":"markdown","source":"********# EARLIER ROUGE ","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import random_split\n\n# Split the dataset into training and testing sets with a ratio of 75% for training and 25% for testing\ntrain_size = int(0.75 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create data loaders for the training and testing sets\n# DataLoader for training set: batch size of 32, shuffling the data, and dropping the last incomplete batch if any\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n# DataLoader for testing set: batch size of 32, without shuffling the data\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# The training dataset is split into a training set and a testing set with a ratio of 75:25.\n# DataLoaders are created for both the training and testing sets, facilitating the efficient loading of batches during training and evaluation.\n# The training DataLoader shuffles the data and drops the last incomplete batch to ensure consistent batch sizes during training.\n# The testing DataLoader does not shuffle the data since the order of data is not relevant during evaluation.\n\n\n\n\n# Define the training function for the model\ndef train(model, optimizer, train_dl, epochs):\n    # Iterate over the specified number of epochs\n    for epoch in range(epochs):\n        model.train()  # Set the model to training mode\n        # Iterate over each batch in the training DataLoader\n        for idx, batch in enumerate(train_dl):\n            optimizer.zero_grad()  # Reset gradients to zero\n            batch = batch.to(device)  # Move batch to the specified device (e.g., GPU)\n            output = model(batch, labels=batch)  # Forward pass: compute model predictions\n            loss = output[0]  # Compute the loss based on model predictions\n            loss.backward()  # Backward pass: compute gradients\n            optimizer.step()  # Update model parameters based on gradients\n            # Print loss every 50 batches\n            if idx % 50 == 0:\n                print(f\"Epoch [{epoch+1}/{epochs}], Step [{idx+1}/{len(train_dl)}], Loss: {loss.item()}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T13:32:17.598539Z","iopub.execute_input":"2024-04-22T13:32:17.598848Z","iopub.status.idle":"2024-04-22T13:32:18.774426Z","shell.execute_reply.started":"2024-04-22T13:32:17.598822Z","shell.execute_reply":"2024-04-22T13:32:18.773637Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Train the model on the training set\ntrain(model, optimizer, train_dataloader, epochs=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T13:32:18.848482Z","iopub.execute_input":"2024-04-22T13:32:18.848729Z","iopub.status.idle":"2024-04-22T15:15:46.943850Z","shell.execute_reply.started":"2024-04-22T13:32:18.848708Z","shell.execute_reply":"2024-04-22T15:15:46.942812Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"Epoch [1/1], Step [1/13323], Loss: 1.8593367338180542\nEpoch [1/1], Step [51/13323], Loss: 1.9994953870773315\nEpoch [1/1], Step [101/13323], Loss: 1.8977000713348389\nEpoch [1/1], Step [151/13323], Loss: 2.0506272315979004\nEpoch [1/1], Step [201/13323], Loss: 1.5420185327529907\nEpoch [1/1], Step [251/13323], Loss: 1.4611223936080933\nEpoch [1/1], Step [301/13323], Loss: 2.0511529445648193\nEpoch [1/1], Step [351/13323], Loss: 1.8443139791488647\nEpoch [1/1], Step [401/13323], Loss: 2.04577898979187\nEpoch [1/1], Step [451/13323], Loss: 2.006317615509033\nEpoch [1/1], Step [501/13323], Loss: 1.8634902238845825\nEpoch [1/1], Step [551/13323], Loss: 2.206110715866089\nEpoch [1/1], Step [601/13323], Loss: 1.6728296279907227\nEpoch [1/1], Step [651/13323], Loss: 1.805537223815918\nEpoch [1/1], Step [701/13323], Loss: 1.817780613899231\nEpoch [1/1], Step [751/13323], Loss: 2.183117389678955\nEpoch [1/1], Step [801/13323], Loss: 2.004399538040161\nEpoch [1/1], Step [851/13323], Loss: 1.8522589206695557\nEpoch [1/1], Step [901/13323], Loss: 2.035400867462158\nEpoch [1/1], Step [951/13323], Loss: 2.096686840057373\nEpoch [1/1], Step [1001/13323], Loss: 2.0694565773010254\nEpoch [1/1], Step [1051/13323], Loss: 2.077425241470337\nEpoch [1/1], Step [1101/13323], Loss: 1.8684288263320923\nEpoch [1/1], Step [1151/13323], Loss: 1.8990898132324219\nEpoch [1/1], Step [1201/13323], Loss: 1.921075463294983\nEpoch [1/1], Step [1251/13323], Loss: 1.8216502666473389\nEpoch [1/1], Step [1301/13323], Loss: 2.0776424407958984\nEpoch [1/1], Step [1351/13323], Loss: 2.150500774383545\nEpoch [1/1], Step [1401/13323], Loss: 2.0110044479370117\nEpoch [1/1], Step [1451/13323], Loss: 1.669289469718933\nEpoch [1/1], Step [1501/13323], Loss: 1.914114236831665\nEpoch [1/1], Step [1551/13323], Loss: 1.8135563135147095\nEpoch [1/1], Step [1601/13323], Loss: 1.8757691383361816\nEpoch [1/1], Step [1651/13323], Loss: 1.7813657522201538\nEpoch [1/1], Step [1701/13323], Loss: 1.8469264507293701\nEpoch [1/1], Step [1751/13323], Loss: 1.8516680002212524\nEpoch [1/1], Step [1801/13323], Loss: 1.8303462266921997\nEpoch [1/1], Step [1851/13323], Loss: 1.9297868013381958\nEpoch [1/1], Step [1901/13323], Loss: 2.013068437576294\nEpoch [1/1], Step [1951/13323], Loss: 1.810950517654419\nEpoch [1/1], Step [2001/13323], Loss: 1.407564640045166\nEpoch [1/1], Step [2051/13323], Loss: 1.9109692573547363\nEpoch [1/1], Step [2101/13323], Loss: 2.1503963470458984\nEpoch [1/1], Step [2151/13323], Loss: 1.6724933385849\nEpoch [1/1], Step [2201/13323], Loss: 2.0952253341674805\nEpoch [1/1], Step [2251/13323], Loss: 1.7710704803466797\nEpoch [1/1], Step [2301/13323], Loss: 1.87917959690094\nEpoch [1/1], Step [2351/13323], Loss: 2.0899248123168945\nEpoch [1/1], Step [2401/13323], Loss: 1.8799912929534912\nEpoch [1/1], Step [2451/13323], Loss: 2.152127981185913\nEpoch [1/1], Step [2501/13323], Loss: 1.814243197441101\nEpoch [1/1], Step [2551/13323], Loss: 1.8825352191925049\nEpoch [1/1], Step [2601/13323], Loss: 1.594435453414917\nEpoch [1/1], Step [2651/13323], Loss: 1.9116588830947876\nEpoch [1/1], Step [2701/13323], Loss: 2.0606493949890137\nEpoch [1/1], Step [2751/13323], Loss: 1.8661918640136719\nEpoch [1/1], Step [2801/13323], Loss: 1.9912052154541016\nEpoch [1/1], Step [2851/13323], Loss: 1.7195786237716675\nEpoch [1/1], Step [2901/13323], Loss: 1.9012670516967773\nEpoch [1/1], Step [2951/13323], Loss: 1.9572492837905884\nEpoch [1/1], Step [3001/13323], Loss: 1.868570327758789\nEpoch [1/1], Step [3051/13323], Loss: 1.8950169086456299\nEpoch [1/1], Step [3101/13323], Loss: 1.8960697650909424\nEpoch [1/1], Step [3151/13323], Loss: 1.6383172273635864\nEpoch [1/1], Step [3201/13323], Loss: 1.7797762155532837\nEpoch [1/1], Step [3251/13323], Loss: 1.8346738815307617\nEpoch [1/1], Step [3301/13323], Loss: 1.9317412376403809\nEpoch [1/1], Step [3351/13323], Loss: 1.7183421850204468\nEpoch [1/1], Step [3401/13323], Loss: 1.9299323558807373\nEpoch [1/1], Step [3451/13323], Loss: 1.964643120765686\nEpoch [1/1], Step [3501/13323], Loss: 1.9855806827545166\nEpoch [1/1], Step [3551/13323], Loss: 1.9137684106826782\nEpoch [1/1], Step [3601/13323], Loss: 1.8044452667236328\nEpoch [1/1], Step [3651/13323], Loss: 1.9294006824493408\nEpoch [1/1], Step [3701/13323], Loss: 2.0807619094848633\nEpoch [1/1], Step [3751/13323], Loss: 1.625539779663086\nEpoch [1/1], Step [3801/13323], Loss: 1.9944671392440796\nEpoch [1/1], Step [3851/13323], Loss: 1.8847376108169556\nEpoch [1/1], Step [3901/13323], Loss: 1.8487818241119385\nEpoch [1/1], Step [3951/13323], Loss: 1.8154064416885376\nEpoch [1/1], Step [4001/13323], Loss: 2.1736629009246826\nEpoch [1/1], Step [4051/13323], Loss: 1.9455980062484741\nEpoch [1/1], Step [4101/13323], Loss: 2.0128092765808105\nEpoch [1/1], Step [4151/13323], Loss: 1.9527301788330078\nEpoch [1/1], Step [4201/13323], Loss: 1.9129643440246582\nEpoch [1/1], Step [4251/13323], Loss: 2.1127841472625732\nEpoch [1/1], Step [4301/13323], Loss: 1.857349157333374\nEpoch [1/1], Step [4351/13323], Loss: 2.178356647491455\nEpoch [1/1], Step [4401/13323], Loss: 1.9009180068969727\nEpoch [1/1], Step [4451/13323], Loss: 2.02463960647583\nEpoch [1/1], Step [4501/13323], Loss: 1.9291868209838867\nEpoch [1/1], Step [4551/13323], Loss: 2.142850399017334\nEpoch [1/1], Step [4601/13323], Loss: 1.8973115682601929\nEpoch [1/1], Step [4651/13323], Loss: 1.913365364074707\nEpoch [1/1], Step [4701/13323], Loss: 1.6300108432769775\nEpoch [1/1], Step [4751/13323], Loss: 1.8966221809387207\nEpoch [1/1], Step [4801/13323], Loss: 2.280146837234497\nEpoch [1/1], Step [4851/13323], Loss: 1.9418772459030151\nEpoch [1/1], Step [4901/13323], Loss: 1.7823336124420166\nEpoch [1/1], Step [4951/13323], Loss: 1.8024250268936157\nEpoch [1/1], Step [5001/13323], Loss: 2.1211421489715576\nEpoch [1/1], Step [5051/13323], Loss: 1.8038098812103271\nEpoch [1/1], Step [5101/13323], Loss: 2.0249433517456055\nEpoch [1/1], Step [5151/13323], Loss: 1.8319696187973022\nEpoch [1/1], Step [5201/13323], Loss: 1.8857462406158447\nEpoch [1/1], Step [5251/13323], Loss: 1.9605473279953003\nEpoch [1/1], Step [5301/13323], Loss: 2.2325241565704346\nEpoch [1/1], Step [5351/13323], Loss: 2.135392665863037\nEpoch [1/1], Step [5401/13323], Loss: 1.8125841617584229\nEpoch [1/1], Step [5451/13323], Loss: 1.9332795143127441\nEpoch [1/1], Step [5501/13323], Loss: 1.950486660003662\nEpoch [1/1], Step [5551/13323], Loss: 1.8388936519622803\nEpoch [1/1], Step [5601/13323], Loss: 2.12807035446167\nEpoch [1/1], Step [5651/13323], Loss: 1.6925292015075684\nEpoch [1/1], Step [5701/13323], Loss: 1.956146001815796\nEpoch [1/1], Step [5751/13323], Loss: 1.864489197731018\nEpoch [1/1], Step [5801/13323], Loss: 1.8688945770263672\nEpoch [1/1], Step [5851/13323], Loss: 1.8936350345611572\nEpoch [1/1], Step [5901/13323], Loss: 2.2162928581237793\nEpoch [1/1], Step [5951/13323], Loss: 1.838510274887085\nEpoch [1/1], Step [6001/13323], Loss: 1.779976725578308\nEpoch [1/1], Step [6051/13323], Loss: 1.80898916721344\nEpoch [1/1], Step [6101/13323], Loss: 1.8144537210464478\nEpoch [1/1], Step [6151/13323], Loss: 1.8902982473373413\nEpoch [1/1], Step [6201/13323], Loss: 2.067850351333618\nEpoch [1/1], Step [6251/13323], Loss: 1.9593560695648193\nEpoch [1/1], Step [6301/13323], Loss: 2.020855665206909\nEpoch [1/1], Step [6351/13323], Loss: 1.914980411529541\nEpoch [1/1], Step [6401/13323], Loss: 1.6727449893951416\nEpoch [1/1], Step [6451/13323], Loss: 2.0174825191497803\nEpoch [1/1], Step [6501/13323], Loss: 1.7671254873275757\nEpoch [1/1], Step [6551/13323], Loss: 2.213585615158081\nEpoch [1/1], Step [6601/13323], Loss: 2.0638558864593506\nEpoch [1/1], Step [6651/13323], Loss: 1.9222606420516968\nEpoch [1/1], Step [6701/13323], Loss: 1.8252533674240112\nEpoch [1/1], Step [6751/13323], Loss: 2.0222110748291016\nEpoch [1/1], Step [6801/13323], Loss: 2.0266642570495605\nEpoch [1/1], Step [6851/13323], Loss: 2.016759157180786\nEpoch [1/1], Step [6901/13323], Loss: 2.0608413219451904\nEpoch [1/1], Step [6951/13323], Loss: 2.0079097747802734\nEpoch [1/1], Step [7001/13323], Loss: 2.135938882827759\nEpoch [1/1], Step [7051/13323], Loss: 1.6093546152114868\nEpoch [1/1], Step [7101/13323], Loss: 1.6394188404083252\nEpoch [1/1], Step [7151/13323], Loss: 1.9668195247650146\nEpoch [1/1], Step [7201/13323], Loss: 1.8977442979812622\nEpoch [1/1], Step [7251/13323], Loss: 1.9204384088516235\nEpoch [1/1], Step [7301/13323], Loss: 1.8027960062026978\nEpoch [1/1], Step [7351/13323], Loss: 1.9839893579483032\nEpoch [1/1], Step [7401/13323], Loss: 2.0090181827545166\nEpoch [1/1], Step [7451/13323], Loss: 1.9405510425567627\nEpoch [1/1], Step [7501/13323], Loss: 1.806592583656311\nEpoch [1/1], Step [7551/13323], Loss: 2.1652944087982178\nEpoch [1/1], Step [7601/13323], Loss: 2.000552177429199\nEpoch [1/1], Step [7651/13323], Loss: 1.9964110851287842\nEpoch [1/1], Step [7701/13323], Loss: 1.9104042053222656\nEpoch [1/1], Step [7751/13323], Loss: 1.7111854553222656\nEpoch [1/1], Step [7801/13323], Loss: 1.6446475982666016\nEpoch [1/1], Step [7851/13323], Loss: 1.9736560583114624\nEpoch [1/1], Step [7901/13323], Loss: 1.8566796779632568\nEpoch [1/1], Step [7951/13323], Loss: 1.9211431741714478\nEpoch [1/1], Step [8001/13323], Loss: 2.0093679428100586\nEpoch [1/1], Step [8051/13323], Loss: 2.0747151374816895\nEpoch [1/1], Step [8101/13323], Loss: 2.04645037651062\nEpoch [1/1], Step [8151/13323], Loss: 1.6799840927124023\nEpoch [1/1], Step [8201/13323], Loss: 1.8120719194412231\nEpoch [1/1], Step [8251/13323], Loss: 1.954738974571228\nEpoch [1/1], Step [8301/13323], Loss: 1.9820940494537354\nEpoch [1/1], Step [8351/13323], Loss: 1.9676356315612793\nEpoch [1/1], Step [8401/13323], Loss: 2.058576822280884\nEpoch [1/1], Step [8451/13323], Loss: 2.2129595279693604\nEpoch [1/1], Step [8501/13323], Loss: 1.9552497863769531\nEpoch [1/1], Step [8551/13323], Loss: 1.8359630107879639\nEpoch [1/1], Step [8601/13323], Loss: 1.9980655908584595\nEpoch [1/1], Step [8651/13323], Loss: 1.960633635520935\nEpoch [1/1], Step [8701/13323], Loss: 1.973235845565796\nEpoch [1/1], Step [8751/13323], Loss: 1.8183228969573975\nEpoch [1/1], Step [8801/13323], Loss: 1.7930341958999634\nEpoch [1/1], Step [8851/13323], Loss: 1.8086961507797241\nEpoch [1/1], Step [8901/13323], Loss: 1.9198353290557861\nEpoch [1/1], Step [8951/13323], Loss: 1.634967565536499\nEpoch [1/1], Step [9001/13323], Loss: 1.6823698282241821\nEpoch [1/1], Step [9051/13323], Loss: 1.5595238208770752\nEpoch [1/1], Step [9101/13323], Loss: 1.905856728553772\nEpoch [1/1], Step [9151/13323], Loss: 1.7964011430740356\nEpoch [1/1], Step [9201/13323], Loss: 2.0394861698150635\nEpoch [1/1], Step [9251/13323], Loss: 1.6553316116333008\nEpoch [1/1], Step [9301/13323], Loss: 1.881771206855774\nEpoch [1/1], Step [9351/13323], Loss: 1.7127324342727661\nEpoch [1/1], Step [9401/13323], Loss: 2.0573887825012207\nEpoch [1/1], Step [9451/13323], Loss: 2.0987372398376465\nEpoch [1/1], Step [9501/13323], Loss: 1.5173234939575195\nEpoch [1/1], Step [9551/13323], Loss: 1.7963298559188843\nEpoch [1/1], Step [9601/13323], Loss: 1.910997986793518\nEpoch [1/1], Step [9651/13323], Loss: 2.147643566131592\nEpoch [1/1], Step [9701/13323], Loss: 1.7826851606369019\nEpoch [1/1], Step [9751/13323], Loss: 2.162510395050049\nEpoch [1/1], Step [9801/13323], Loss: 2.167438507080078\nEpoch [1/1], Step [9851/13323], Loss: 1.6650397777557373\nEpoch [1/1], Step [9901/13323], Loss: 1.9039068222045898\nEpoch [1/1], Step [9951/13323], Loss: 1.7916259765625\nEpoch [1/1], Step [10001/13323], Loss: 2.0217456817626953\nEpoch [1/1], Step [10051/13323], Loss: 1.981878638267517\nEpoch [1/1], Step [10101/13323], Loss: 1.8686474561691284\nEpoch [1/1], Step [10151/13323], Loss: 1.8873112201690674\nEpoch [1/1], Step [10201/13323], Loss: 1.7482918500900269\nEpoch [1/1], Step [10251/13323], Loss: 1.8134238719940186\nEpoch [1/1], Step [10301/13323], Loss: 1.646714687347412\nEpoch [1/1], Step [10351/13323], Loss: 1.7761223316192627\nEpoch [1/1], Step [10401/13323], Loss: 1.917398452758789\nEpoch [1/1], Step [10451/13323], Loss: 1.9740629196166992\nEpoch [1/1], Step [10501/13323], Loss: 1.8596736192703247\nEpoch [1/1], Step [10551/13323], Loss: 1.9144370555877686\nEpoch [1/1], Step [10601/13323], Loss: 1.6172658205032349\nEpoch [1/1], Step [10651/13323], Loss: 1.6614508628845215\nEpoch [1/1], Step [10701/13323], Loss: 1.906051516532898\nEpoch [1/1], Step [10751/13323], Loss: 2.0850167274475098\nEpoch [1/1], Step [10801/13323], Loss: 1.9570807218551636\nEpoch [1/1], Step [10851/13323], Loss: 1.7736315727233887\nEpoch [1/1], Step [10901/13323], Loss: 2.1388654708862305\nEpoch [1/1], Step [10951/13323], Loss: 2.089226484298706\nEpoch [1/1], Step [11001/13323], Loss: 1.9480562210083008\nEpoch [1/1], Step [11051/13323], Loss: 1.9375838041305542\nEpoch [1/1], Step [11101/13323], Loss: 2.136888027191162\nEpoch [1/1], Step [11151/13323], Loss: 1.8061046600341797\nEpoch [1/1], Step [11201/13323], Loss: 1.9908589124679565\nEpoch [1/1], Step [11251/13323], Loss: 1.9219461679458618\nEpoch [1/1], Step [11301/13323], Loss: 1.8890776634216309\nEpoch [1/1], Step [11351/13323], Loss: 1.9788035154342651\nEpoch [1/1], Step [11401/13323], Loss: 1.5357309579849243\nEpoch [1/1], Step [11451/13323], Loss: 1.8916996717453003\nEpoch [1/1], Step [11501/13323], Loss: 1.9336580038070679\nEpoch [1/1], Step [11551/13323], Loss: 1.730126142501831\nEpoch [1/1], Step [11601/13323], Loss: 2.1746695041656494\nEpoch [1/1], Step [11651/13323], Loss: 1.9233794212341309\nEpoch [1/1], Step [11701/13323], Loss: 2.0420734882354736\nEpoch [1/1], Step [11751/13323], Loss: 2.2696192264556885\nEpoch [1/1], Step [11801/13323], Loss: 1.8142954111099243\nEpoch [1/1], Step [11851/13323], Loss: 1.9056501388549805\nEpoch [1/1], Step [11901/13323], Loss: 2.222043752670288\nEpoch [1/1], Step [11951/13323], Loss: 1.994773507118225\nEpoch [1/1], Step [12001/13323], Loss: 2.131202459335327\nEpoch [1/1], Step [12051/13323], Loss: 2.012481212615967\nEpoch [1/1], Step [12101/13323], Loss: 1.7984973192214966\nEpoch [1/1], Step [12151/13323], Loss: 1.6288018226623535\nEpoch [1/1], Step [12201/13323], Loss: 2.115076780319214\nEpoch [1/1], Step [12251/13323], Loss: 1.8810100555419922\nEpoch [1/1], Step [12301/13323], Loss: 2.284257173538208\nEpoch [1/1], Step [12351/13323], Loss: 1.9468507766723633\nEpoch [1/1], Step [12401/13323], Loss: 2.0090150833129883\nEpoch [1/1], Step [12451/13323], Loss: 1.8493815660476685\nEpoch [1/1], Step [12501/13323], Loss: 1.8296180963516235\nEpoch [1/1], Step [12551/13323], Loss: 1.740234375\nEpoch [1/1], Step [12601/13323], Loss: 1.7949384450912476\nEpoch [1/1], Step [12651/13323], Loss: 2.080493450164795\nEpoch [1/1], Step [12701/13323], Loss: 1.6671286821365356\nEpoch [1/1], Step [12751/13323], Loss: 2.0497944355010986\nEpoch [1/1], Step [12801/13323], Loss: 1.820830225944519\nEpoch [1/1], Step [12851/13323], Loss: 1.7951972484588623\nEpoch [1/1], Step [12901/13323], Loss: 1.9189783334732056\nEpoch [1/1], Step [12951/13323], Loss: 1.8527307510375977\nEpoch [1/1], Step [13001/13323], Loss: 1.6524192094802856\nEpoch [1/1], Step [13051/13323], Loss: 1.9082450866699219\nEpoch [1/1], Step [13101/13323], Loss: 2.0996479988098145\nEpoch [1/1], Step [13151/13323], Loss: 1.6091285943984985\nEpoch [1/1], Step [13201/13323], Loss: 2.0307776927948\nEpoch [1/1], Step [13251/13323], Loss: 1.7898027896881104\nEpoch [1/1], Step [13301/13323], Loss: 1.7528245449066162\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(model_path))\nmodel.eval()  # Set the model to evaluation mode if needed","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:15:48.832400Z","iopub.execute_input":"2024-04-22T15:15:48.832764Z","iopub.status.idle":"2024-04-22T15:15:49.202944Z","shell.execute_reply.started":"2024-04-22T15:15:48.832726Z","shell.execute_reply":"2024-04-22T15:15:49.202045Z"},"trusted":true},"execution_count":82,"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pip install rouge\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:42:25.665116Z","iopub.execute_input":"2024-04-22T16:42:25.665505Z","iopub.status.idle":"2024-04-22T16:42:25.670422Z","shell.execute_reply.started":"2024-04-22T16:42:25.665469Z","shell.execute_reply":"2024-04-22T16:42:25.669322Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install evaluate\n!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:41:45.244924Z","iopub.execute_input":"2024-04-22T16:41:45.245308Z","iopub.status.idle":"2024-04-22T16:42:12.216521Z","shell.execute_reply.started":"2024-04-22T16:41:45.245279Z","shell.execute_reply":"2024-04-22T16:42:12.215436Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load('rouge')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:42:28.990985Z","iopub.execute_input":"2024-04-22T16:42:28.991755Z","iopub.status.idle":"2024-04-22T16:42:29.596514Z","shell.execute_reply.started":"2024-04-22T16:42:28.991724Z","shell.execute_reply":"2024-04-22T16:42:29.595751Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:04:38.948108Z","iopub.execute_input":"2024-04-22T18:04:38.949018Z","iopub.status.idle":"2024-04-22T18:04:39.083957Z","shell.execute_reply.started":"2024-04-22T18:04:38.948980Z","shell.execute_reply":"2024-04-22T18:04:39.083047Z"},"trusted":true},"execution_count":131,"outputs":[{"execution_count":131,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize lists to store Rouge scores for each batch\nrouge1 = []\nrouge2 = []\nrougeL = []\nrougeLsum = []\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Iterate over each batch in the test DataLoader\nfor batch in test_dataloader:\n    # Move the batch to the specified device (e.g., GPU)\n    batch = batch.to(device)\n    \n    # Disable gradient computation during inference\n    with torch.no_grad():\n        # Forward pass: compute model predictions\n        outputs = model(batch)\n\n    # Get the logits from the model outputs\n    logits = outputs.logits\n    \n    # Get the predicted tokens by selecting the token index with the highest probability\n    predictions = torch.argmax(logits, dim=-1)\n    \n    # Decode the generated output sequence, removing special tokens\n    generated_output = tokenizer.decode(predictions[0], skip_special_tokens=True)\n    \n    # Decode the reference sequence from the batch, removing special tokens\n    references = tokenizer.decode(batch[0], skip_special_tokens=True)\n    \n    # Compute Rouge scores between the generated output and reference sequences\n    results = rouge.compute(predictions=[generated_output], references=[references])\n    \n    # Append the Rouge scores to the respective lists\n    rouge1.append(results['rouge1'])\n    rouge2.append(results['rouge2'])\n    rougeL.append(results['rougeL'])\n    rougeLsum.append(results['rougeLsum'])\n\n\nprint(f'rouge1: {sum(rouge1)/len(rouge1)}\\nrouge2: {sum(rouge2)/len(rouge2)}\\nrougeL: {sum(rougeL)/len(rougeL)}\\nrougeLsum: {sum(rougeLsum)/len(rougeLsum)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:34:46.270212Z","iopub.execute_input":"2024-04-22T18:34:46.270962Z","iopub.status.idle":"2024-04-22T18:34:46.277626Z","shell.execute_reply.started":"2024-04-22T18:34:46.270930Z","shell.execute_reply":"2024-04-22T18:34:46.276629Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"rouge1: 0.4086952614760332\nrouge2: 0.0852688926207635\nrougeL: 0.2926001694066451\nrougeLsum: 0.297603059631863\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"inp = input(\"Enter Review text: \")\nmodel.eval()\n\n# Tokenize the input review text using the tokenizer\n# Set 'truncation=True' to truncate the input sequence if it exceeds the maximum length\n# 'return_tensors=\"pt\"' returns PyTorch tensors\n# Move the input tensors to the specified device (e.g., GPU)\ninp = tokenizer(inp, truncation=True, return_tensors=\"pt\").to(device)\n\n# Generate a summary for the input review using the model\n# 'generate' method generates sequences based on the input tensor\n# 'max_length=60' specifies the maximum length of the generated sequence\noutput = model.generate(inp['input_ids'], max_length=60)\ngenerated_output = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(f'Generate Summary: \\n{generated_output}')\n\n\n\n# Import necessary libraries\nfrom rouge import Rouge\nimport pandas as pd\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n# Define functions to calculate ROUGE scores and generate summaries\n\n# Function to calculate ROUGE scores between a generated summary and a reference summary\ndef calculate_rouge(hypothesis, reference):\n    rouge = Rouge()\n    scores = rouge.get_scores(hypothesis, reference)\n    return scores[0]\n\n# Function to generate a summary for a given review text using the GPT-2 model\ndef generate_summary(review_text):\n    # Tokenize the review text and encode it into input tensors\n    inputs = tokenizer.encode(\"summarize: \" + review_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n    \n    # Generate summary using the GPT-2 model\n    summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n    \n    # Decode the generated summary from the output tensor\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\n# Iterate over each row in the reviews DataFrame to generate summaries and calculate ROUGE scores\nfor index, row in reviews_path.iterrows():\n    # Extract review text and reference summary \n    review_text = row[\"Review Text\"]\n    reference_summary = row[\"Summary\"]\n    generated_summary = generate_summary(review_text)\n    \n    # Calculate ROUGE scores between the generated summary and reference summary\n    rouge_scores = calculate_rouge(generated_summary, reference_summary)\n    print(\"Review Text:\", review_text)\n    print(\"Reference Summary:\", reference_summary)\n    print(\"Generated Summary:\", generated_summary)\n    print(\"ROUGE Scores:\", rouge_scores)\n    print(\"=\"*50)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T18:53:07.921811Z","iopub.execute_input":"2024-04-22T18:53:07.922723Z","iopub.status.idle":"2024-04-22T18:53:10.972987Z","shell.execute_reply.started":"2024-04-22T18:53:07.922687Z","shell.execute_reply":"2024-04-22T18:53:10.972101Z"},"trusted":true},"execution_count":159,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter Review text:  The Fender CD-60S Dreadnought Acoustic Guitar is a great instrument for beginners. It has a solid construction, produces a rich sound, and feels comfortable to play. However, some users have reported issues with the tuning stability.\n"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generate Summary: \nThe Fender CD-60S Dreadnought Acoustic Guitar is a great instrument for beginners. It has a solid construction, produces a rich sound, and feels comfortable to play. However, some users have reported issues with the tuning stability.\n\nThe Fender CD-60S Dreadn\n","output_type":"stream"}]},{"cell_type":"code","source":"def topk(probs, n=9):\n    # The scores are initially softmaxed to convert to probabilities\n    probs = torch.softmax(probs, dim= -1)\n    \n    # PyTorch has its own topk method, which we use here\n    tokensProb, topIx = torch.topk(probs, k=n)\n    \n    # The new selection pool (9 choices) is normalized\n    tokensProb = tokensProb / torch.sum(tokensProb)\n\n    # Send to CPU for numpy handling\n    tokensProb = tokensProb.cpu().detach().numpy()\n\n    # Make a random choice from the pool based on the new prob distribution\n    choice = np.random.choice(n, 1, p = tokensProb)\n    tokenId = topIx[choice][0]\n\n    return int(tokenId)\n\ndef model_infer(model, tokenizer, review, max_length=15):\n    # Preprocess the init token (task designator)\n    review_encoded = tokenizer.encode(review)\n    result = review_encoded\n    initial_input = torch.tensor(review_encoded).unsqueeze(0).to(device)\n\n    with torch.set_grad_enabled(False):\n        # Feed the init token to the model\n        output = model(initial_input)\n\n        # Flatten the logits at the final time step\n        logits = output.logits[0,-1]\n\n        # Make a top-k choice and append to the result\n        result.append(topk(logits))\n\n        # For max_length times:\n        for _ in range(max_length):\n            # Feed the current sequence to the model and make a choice\n            input = torch.tensor(result).unsqueeze(0).to(device)\n            output = model(input)\n            logits = output.logits[0,-1]\n            res_id = topk(logits)\n\n            # If the chosen token is EOS, return the result\n            if res_id == tokenizer.eos_token_id:\n                return tokenizer.decode(result)\n            else: # Append to the sequence \n                result.append(res_id)\n    # IF no EOS is generated, return after the max_len\n    return tokenizer.decode(result)\n\nsample_reviews = [review.split(\" TL;DR \")[0] for review in random.sample(reviews, 5)]\nsample_reviews\n\nfor review in sample_reviews:\n    summaries = set()\n    print(review)\n    while len(summaries) < 3:\n        summary = model_infer(model, tokenizer, review + \" TL;DR \").split(\" TL;DR \")[1].strip()\n        if summary not in summaries:\n            summaries.add(summary)\n    print(\"Summaries: \"+ str(summaries) +\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T16:35:14.151281Z","iopub.execute_input":"2024-04-22T16:35:14.152120Z","iopub.status.idle":"2024-04-22T16:35:14.159014Z","shell.execute_reply.started":"2024-04-22T16:35:14.152086Z","shell.execute_reply":"2024-04-22T16:35:14.158071Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}